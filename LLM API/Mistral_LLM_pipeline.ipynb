{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c4d853d",
   "metadata": {},
   "source": [
    "# LLM Pipeline for Mistral models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "099a5f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import MarkupResemblesLocatorWarning\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import backoff\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "from langchain_mistralai import ChatMistralAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a55067",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"MISTRAL_API_KEY environment variable is not set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec28dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_cleaning(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  \n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)          \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()             \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73642e12",
   "metadata": {},
   "source": [
    "## Initialize Mistral Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74dd0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailClassificationResponse(BaseModel):\n",
    "    classification: int = Field(description=\"0 for benign, 1 for phishing\")\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=20,\n",
    "    check_every_n_seconds=5,\n",
    "    max_bucket_size=40,\n",
    ")\n",
    "\n",
    "model = \"mistral-large-latest\"\n",
    "client = ChatMistralAI(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"MISTRAL_API_KEY\"),\n",
    "    rate_limiter=rate_limiter\n",
    ")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=EmailClassificationResponse)\n",
    "client = client.with_structured_output(EmailClassificationResponse, method=\"json_mode\")\n",
    "\n",
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    Exception,\n",
    "    max_tries=3,\n",
    "    jitter=backoff.full_jitter,\n",
    ")\n",
    "def call_llm(prompt):\n",
    "    return client.invoke(prompt)\n",
    "\n",
    "def classify_email_numeric(row):\n",
    "    subject = row[\"subject\"]\n",
    "    text = row[\"text\"]\n",
    "    true_label = int(row[\"label\"])\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are a cybersecurity expert. Classify the following email as phishing (1) or benign (0). \"\n",
    "        f\"Output only a JSON with the integer field 'classification'. \"\n",
    "        f\"Base your decision on the subject and body only. Mark as phishing if the content is suspicious, manipulative, unexpected, or spam-like. \"\n",
    "        f\"Look for urgency, requests for credentials, threatening tone, excessive capitalization, odd formatting, or pressure to act. \"\n",
    "        f\"Note that modern phishing emails may not always follow traditional patterns and can be more targeted towards the receiver.\\n\\n\"\n",
    "        f\"Subject: {subject}\\n\\n\"\n",
    "        f\"Body:\\n{text}\\n\\n\"\n",
    "        f\"{parser.get_format_instructions()}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = call_llm(prompt)\n",
    "        return {\n",
    "            \"true_label\": true_label,\n",
    "            \"predicted_label\": response.classification\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e4c06",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeea608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppress spurious BeautifulSoup warnings\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "\n",
    "#cleaning function for email subject and body\n",
    "def bert_cleaning(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  \n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)          \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()             \n",
    "    return text\n",
    "\n",
    "folder_path = \"../Datasets/processed_datasets\"\n",
    "full_datasets = {}\n",
    "\n",
    "#load datasets from CSV files and apply cleaning + filtering\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        name = file.removesuffix(\".csv\")\n",
    "        df = pd.read_csv(os.path.join(folder_path, file))\n",
    "\n",
    "        # clean labels, ensuring they are integers\n",
    "        df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"label\"]).copy()\n",
    "        df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "        #only keep entries with valid subject and text\n",
    "        df = df[\n",
    "            df[\"subject\"].apply(lambda x: isinstance(x, str) and x.strip() != \"\") &\n",
    "            df[\"text\"].apply(lambda x: isinstance(x, str) and x.strip() != \"\")\n",
    "        ]\n",
    "\n",
    "        #clean subject and text separately\n",
    "        df[\"subject\"] = df[\"subject\"].apply(bert_cleaning)\n",
    "        df[\"text\"] = df[\"text\"].apply(bert_cleaning)\n",
    "\n",
    "        #remove duplicates based on subject and text\n",
    "        df = df.drop_duplicates(subset=[\"subject\", \"text\"]).reset_index(drop=True)\n",
    "\n",
    "        #ensure both label classes are present\n",
    "        label_counts = df[\"label\"].value_counts()\n",
    "        if 0 in label_counts and 1 in label_counts:\n",
    "            full_datasets[name] = df[[\"subject\", \"text\", \"label\"]]\n",
    "            print(f\"{name} accepted: {len(df)} samples\")\n",
    "        else:\n",
    "            print(f\"{name} skipped: missing class 0 or 1\")\n",
    "\n",
    "print(f\"\\nLoaded and cleaned {len(full_datasets)} datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6cab81",
   "metadata": {},
   "source": [
    "## Load Caripoti dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Datasets/13_Pajola/emails_with_languages.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df_caripoti = pd.DataFrame(data)\n",
    "df_caripoti = df_caripoti[[\"Subject\", \"Body\", \"type\", \"Language\"]]\n",
    "\n",
    "#filter for English language only\n",
    "df_caripoti = df_caripoti[df_caripoti[\"Language\"] == \"en\"].copy()\n",
    "\n",
    "#ensure consistent column names\n",
    "df_caripoti = df_caripoti.rename(columns={\n",
    "    \"Subject\": \"subject\",\n",
    "    \"Body\": \"text\",\n",
    "    \"type\": \"label\"\n",
    "})\n",
    "\n",
    "#ensure labels are int, and drop NaNs to avoid issues\n",
    "df_caripoti[\"label\"] = pd.to_numeric(df_caripoti[\"label\"], errors=\"coerce\")\n",
    "df_caripoti = df_caripoti.dropna(subset=[\"label\"])\n",
    "df_caripoti[\"label\"] = df_caripoti[\"label\"].astype(int)\n",
    "\n",
    "#filter out empty rows\n",
    "df_caripoti = df_caripoti[\n",
    "    df_caripoti[\"subject\"].apply(lambda x: isinstance(x, str) and x.strip() != \"\") &\n",
    "    df_caripoti[\"text\"].apply(lambda x: isinstance(x, str) and x.strip() != \"\")\n",
    "]\n",
    "\n",
    "#clean with bert_cleaning function\n",
    "df_caripoti[\"subject\"] = df_caripoti[\"subject\"].apply(bert_cleaning)\n",
    "df_caripoti[\"text\"] = df_caripoti[\"text\"].apply(bert_cleaning)\n",
    "\n",
    "#remove duplicate values\n",
    "df_caripoti = df_caripoti.drop_duplicates(subset=[\"subject\", \"text\"]).reset_index(drop=True)\n",
    "\n",
    "#check if both classes are present\n",
    "label_counts = df_caripoti[\"label\"].value_counts()\n",
    "if 0 in label_counts and 1 in label_counts:\n",
    "    full_datasets[\"Caripoti\"] = df_caripoti[[\"subject\", \"text\", \"label\"]]\n",
    "    print(f\"Caripoti accepted: {len(df_caripoti)} samples\")\n",
    "else:\n",
    "    print(\"Caripoti skipped: missing class 0 or 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbc318c",
   "metadata": {},
   "source": [
    "## Evaluate on single full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d347f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full dataset separately\n",
    "full_dataset_no_split = {\n",
    "    dataset_name: dataset_df\n",
    "    for dataset_name, dataset_df in full_datasets.items()\n",
    "    if dataset_name == \"Chatuat_Enhancing_Phishing_Detection\"\n",
    "}\n",
    "\n",
    "results_dir = \"llm_eval_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "safe_model_name = model.replace(\"/\", \"-\")\n",
    "\n",
    "#evaluate on the full dataset without splitting\n",
    "for name, df in full_dataset_no_split.items():\n",
    "    print(f\"\\nProcessing full dataset: {name}\")\n",
    "\n",
    "    test_df = df.reset_index(drop=True)\n",
    "\n",
    "    #check if results file already exists and resume from where the evaluation was interrupted\n",
    "    result_path = os.path.join(results_dir, f\"{name}_{safe_model_name}_full_results.csv\")\n",
    "    \n",
    "    if os.path.exists(result_path):\n",
    "        try:\n",
    "            existing_df = pd.read_csv(result_path)\n",
    "            results = existing_df.to_dict(\"records\")\n",
    "            start_idx = len(existing_df)\n",
    "            print(f\"Resuming from sample {start_idx}/{len(test_df)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read existing results from {result_path}. Starting from scratch.\")\n",
    "            results = []\n",
    "            start_idx = 0\n",
    "    else:\n",
    "        results = []\n",
    "        start_idx = 0\n",
    "\n",
    "    #classification for each email\n",
    "    for i in range(start_idx, len(test_df)):\n",
    "        row = test_df.iloc[i]\n",
    "        result = classify_email_numeric(row)\n",
    "        result[\"index\"] = i\n",
    "\n",
    "        if \"error\" not in result:\n",
    "            results.append(result)\n",
    "\n",
    "            if (i + 1) % 250 == 0 or (i + 1) == len(test_df):\n",
    "                results_df = pd.DataFrame(results)\n",
    "                results_df.to_csv(result_path, index=False)\n",
    "                print(f\"Progress saved at sample {i + 1}\")\n",
    "        else:\n",
    "            print(f\"Error at index {i}: {result['error']}\")\n",
    "            time.sleep(5)\n",
    "\n",
    "    #evaluate results\n",
    "    eval_df = pd.DataFrame(results)\n",
    "    acc = accuracy_score(eval_df[\"true_label\"], eval_df[\"predicted_label\"])\n",
    "    prec = precision_score(eval_df[\"true_label\"], eval_df[\"predicted_label\"])\n",
    "    rec = recall_score(eval_df[\"true_label\"], eval_df[\"predicted_label\"])\n",
    "    f1 = f1_score(eval_df[\"true_label\"], eval_df[\"predicted_label\"])\n",
    "\n",
    "    #save\n",
    "    llm_split_results_df = pd.DataFrame({\n",
    "        \"dataset\": [f\"{name}_Full\"],\n",
    "        \"model\": [model],\n",
    "        \"accuracy\": [acc],\n",
    "        \"precision\": [prec],\n",
    "        \"recall\": [rec],\n",
    "        \"f1_score\": [f1],\n",
    "        \"num_samples\": [len(eval_df)],\n",
    "    })\n",
    "\n",
    "    llm_split_results_path = os.path.join(results_dir, \"llm_split_results.csv\")\n",
    "    llm_split_results_df.to_csv(\n",
    "        llm_split_results_path,\n",
    "        mode=\"a\",\n",
    "        index=False,\n",
    "        header=not os.path.exists(llm_split_results_path)\n",
    "    )\n",
    "\n",
    "    print(f\"Full dataset results appended for {name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
