{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebefa09f",
   "metadata": {},
   "source": [
    "# DistilBERT 5seed pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4615e5",
   "metadata": {},
   "source": [
    "*This code was used in Google Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d104ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_original_array = np.array\n",
    "def safe_array(obj, *args, **kwargs):\n",
    "    if \"copy\" in kwargs and kwargs[\"copy\"] is False:\n",
    "        return np.asarray(obj)\n",
    "    return _original_array(obj, *args, **kwargs)\n",
    "\n",
    "np.array = safe_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72092801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets, load_dataset, Dataset, concatenate_datasets, ClassLabel, DatasetDict\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments, DistilBertTokenizerFast, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af54d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset_dir = \"/content/drive/MyDrive/MASTER EXPERIMENTS/original_datasets\"\n",
    "output_base_dir = \"/content/drive/MyDrive/MASTER EXPERIMENTS/bert_allvsone_5seed_output\"\n",
    "\n",
    "#sub dirs\n",
    "model_dir   = os.path.join(output_base_dir, \"models\")\n",
    "results_dir = os.path.join(output_base_dir, \"results\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "#result files\n",
    "internal_results_csv = os.path.join(results_dir, \"internal_eval.csv\")\n",
    "cross_results_csv    = os.path.join(results_dir, \"cross_eval.csv\")\n",
    "\n",
    "#helper for model paths per datasets and seed Modellpfade\n",
    "def model_dir_for(dataset_name, seed):\n",
    "    d = os.path.join(model_dir, dataset_name, f\"seed_{seed}\")\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "    return d\n",
    "\n",
    "print(\"Structure created\")\n",
    "print(\"Models base:\", model_dir)\n",
    "print(\"Results base:\", results_dir)\n",
    "print(\"Internal CSV:\", internal_results_csv)\n",
    "print(\"Cross CSV:\", cross_results_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6833cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c39cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    texts = [text if isinstance(text, str) else \"\" for text in batch[\"text\"]]\n",
    "    return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "def prepare_dataset(ds):\n",
    "    columns_to_remove = [col for col in ds.column_names if col not in [\"input_ids\", \"attention_mask\", \"label\"]]\n",
    "    if columns_to_remove:\n",
    "        ds = ds.remove_columns(columns_to_remove)\n",
    "    return ds.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_list = [7, 28, 42, 95, 450]\n",
    "tokenized_sets = []\n",
    "\n",
    "# load datasets\n",
    "dataset_files = [f for f in os.listdir(base_dataset_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "for file in dataset_files:\n",
    "    name = os.path.splitext(file)[0]\n",
    "    print(f\"Loading: {name}\")\n",
    "\n",
    "    # load csv to pandas df\n",
    "    df = pd.read_csv(os.path.join(base_dataset_dir, file))\n",
    "\n",
    "    # clean up dataframe\n",
    "    df = df[df[\"text\"].apply(lambda x: isinstance(x, str) and x.strip() != \"\")]\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "    df = df[[\"text\", \"label\"]].dropna().reset_index(drop=True)\n",
    "\n",
    "    # dicts for all seeds\n",
    "    train_dict, val_dict = {}, {}\n",
    "\n",
    "    for seed in seed_list:\n",
    "        # create train / val split for each seed\n",
    "        train_df, val_df = train_test_split(\n",
    "            df,\n",
    "            test_size=0.3,\n",
    "            stratify=df[\"label\"],\n",
    "            random_state=seed\n",
    "        )\n",
    "\n",
    "        # convert to hugging face dataset\n",
    "        train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "        val_ds = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "\n",
    "        # tokenize datasets and prepare\n",
    "        tokenized_train = prepare_dataset(train_ds.map(tokenize_function, batched=True))\n",
    "        tokenized_val = prepare_dataset(val_ds.map(tokenize_function, batched=True))\n",
    "\n",
    "        train_dict[seed] = tokenized_train\n",
    "        val_dict[seed] = tokenized_val\n",
    "\n",
    "        print(f\" -> Prepared seed {seed} for dataset {name}\")\n",
    "\n",
    "    tokenized_sets.append({\n",
    "        \"name\": name,\n",
    "        \"train\": train_dict,\n",
    "        \"val\": val_dict\n",
    "    })\n",
    "\n",
    "print(f\"\\nLoaded and tokenized {len(tokenized_sets)} datasets with {len(seed_list)} seeds each.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a495c59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds),\n",
    "        \"recall\": recall_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5906b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_template = {\n",
    "    \"eval_strategy\": \"epoch\",\n",
    "    \"num_train_epochs\": 6,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"per_device_train_batch_size\": 32,\n",
    "    \"per_device_eval_batch_size\": 64,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"logging_strategy\": \"epoch\",\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"eval_f1\",\n",
    "    \"save_total_limit\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb8c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_single_eval(dataset_name, seed, train_ds, val_ds, training_args_template, model_dir):\n",
    "\n",
    "    #make model output directory unique per dataset+seed\n",
    "    model_output_dir = os.path.join(model_dir, f\"bert_{dataset_name}_seed{seed}\")\n",
    "\n",
    "    #if model exists, load it\n",
    "    if os.path.exists(model_output_dir) and os.path.isfile(os.path.join(model_output_dir, \"model.safetensors\")):\n",
    "        print(f\"[Seed {seed}] Model already exists at: {model_output_dir}. Resuming without retraining.\")\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(model_output_dir)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=model_output_dir,\n",
    "            report_to=\"none\",\n",
    "            seed=seed, \n",
    "            **training_args_template\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            eval_dataset=val_ds,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        return trainer\n",
    "\n",
    "    #train model if it doesnt exist yet\n",
    "    print(f\"[Seed {seed}] Training on dataset: {dataset_name}\")\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_dir,\n",
    "        logging_dir=os.path.join(model_output_dir, \"logs\"),\n",
    "        report_to=\"none\",\n",
    "        seed=seed, \n",
    "        **training_args_template\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    model.save_pretrained(model_output_dir)\n",
    "    tokenizer.save_pretrained(model_output_dir)\n",
    "\n",
    "    print(f\"[Seed {seed}] Finished training on {dataset_name}. Model saved to: {model_output_dir}\")\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ab2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = os.path.join(results_dir, f\"single_eval_results.csv\")\n",
    "\n",
    "#load existing results if available\n",
    "if os.path.exists(results_path):\n",
    "    results_df = pd.read_csv(results_path)\n",
    "    print(f\"Loaded existing results with {len(results_df)} entries from {results_path}\")\n",
    "else:\n",
    "    results_df = pd.DataFrame(columns=[\n",
    "        \"Trained Dataset\", \"Trained Seed\",\n",
    "        \"Tested Dataset\", \"Tested Seed\",\n",
    "        \"Evaluation Type\",\n",
    "        \"Train Size\", \"Test Size\",\n",
    "        \"Accuracy\", \"Precision\", \"Recall\", \"F1\"\n",
    "    ])\n",
    "\n",
    "results = results_df.to_dict(\"records\")  #start with existing results\n",
    "\n",
    "for train_entry in tokenized_sets:\n",
    "    dataset_name = train_entry[\"name\"]\n",
    "\n",
    "    for seed in seed_list:\n",
    "        print(f\"\\nTraining + evaluation: dataset={dataset_name}, seed={seed}\")\n",
    "\n",
    "        train_ds = train_entry[\"train\"][seed]\n",
    "        val_ds = train_entry[\"val\"][seed]\n",
    "\n",
    "        #check if internal evaluation already exists\n",
    "        already_done_internal = (\n",
    "            (results_df[\"Trained Dataset\"] == dataset_name) &\n",
    "            (results_df[\"Trained Seed\"] == seed) &\n",
    "            (results_df[\"Tested Dataset\"] == dataset_name) &\n",
    "            (results_df[\"Tested Seed\"] == seed) &\n",
    "            (results_df[\"Evaluation Type\"] == \"Internal\")\n",
    "        ).any()\n",
    "\n",
    "        #train or resume model (needed for internal + external eval)\n",
    "        trainer = train_model_single_eval(dataset_name, seed, train_ds, val_ds, training_args_template, model_dir)\n",
    "\n",
    "        #internal evaluation\n",
    "        if not already_done_internal:\n",
    "            print(f\"[Seed {seed}] Internal evaluation for: {dataset_name}\")\n",
    "            val_predictions = trainer.predict(val_ds)\n",
    "            val_preds = np.argmax(val_predictions.predictions, axis=1)\n",
    "            val_labels = val_predictions.label_ids\n",
    "\n",
    "            results.append({\n",
    "                \"Trained Dataset\": dataset_name,\n",
    "                \"Trained Seed\": seed,\n",
    "                \"Tested Dataset\": dataset_name,\n",
    "                \"Tested Seed\": seed,\n",
    "                \"Evaluation Type\": \"Internal\",\n",
    "                \"Train Size\": len(train_ds),\n",
    "                \"Test Size\": len(val_ds),\n",
    "                \"Accuracy\": accuracy_score(val_labels, val_preds),\n",
    "                \"Precision\": precision_score(val_labels, val_preds, zero_division=0),\n",
    "                \"Recall\": recall_score(val_labels, val_preds, zero_division=0),\n",
    "                \"F1\": f1_score(val_labels, val_preds, zero_division=0)\n",
    "            })\n",
    "\n",
    "            pd.DataFrame(results).to_csv(results_path, index=False)\n",
    "            results_df = pd.read_csv(results_path)  # reload after saving\n",
    "        else:\n",
    "            print(f\"[Seed {seed}] Internal evaluation already exists → skipping\")\n",
    "\n",
    "        #external evaluation\n",
    "        for test_entry in tokenized_sets:\n",
    "            test_dataset_name = test_entry[\"name\"]\n",
    "\n",
    "            for test_seed in seed_list:\n",
    "                if test_dataset_name == dataset_name and test_seed == seed:\n",
    "                    continue  #skip identical dataset+seed\n",
    "\n",
    "                already_done_external = (\n",
    "                    (results_df[\"Trained Dataset\"] == dataset_name) &\n",
    "                    (results_df[\"Trained Seed\"] == seed) &\n",
    "                    (results_df[\"Tested Dataset\"] == test_dataset_name) &\n",
    "                    (results_df[\"Tested Seed\"] == test_seed) &\n",
    "                    (results_df[\"Evaluation Type\"] == \"External\")\n",
    "                ).any()\n",
    "\n",
    "                if already_done_external:\n",
    "                    print(f\"[Seed {seed}] External evaluation train={dataset_name}, test={test_dataset_name} (seed {test_seed}) already exists, skipping\")\n",
    "                    continue\n",
    "\n",
    "                external_test_ds = test_entry[\"val\"][test_seed]\n",
    "\n",
    "                print(f\"[Seed {seed}] External evaluation: train={dataset_name} (seed {seed}), test={test_dataset_name} (seed {test_seed})\")\n",
    "                test_predictions = trainer.predict(external_test_ds)\n",
    "                test_preds = np.argmax(test_predictions.predictions, axis=1)\n",
    "                test_labels = test_predictions.label_ids\n",
    "\n",
    "                results.append({\n",
    "                    \"Trained Dataset\": dataset_name,\n",
    "                    \"Trained Seed\": seed,\n",
    "                    \"Tested Dataset\": test_dataset_name,\n",
    "                    \"Tested Seed\": test_seed,\n",
    "                    \"Evaluation Type\": \"External\",\n",
    "                    \"Train Size\": len(train_ds),\n",
    "                    \"Test Size\": len(external_test_ds),\n",
    "                    \"Accuracy\": accuracy_score(test_labels, test_preds),\n",
    "                    \"Precision\": precision_score(test_labels, test_preds, zero_division=0),\n",
    "                    \"Recall\": recall_score(test_labels, test_preds, zero_division=0),\n",
    "                    \"F1\": f1_score(test_labels, test_preds, zero_division=0)\n",
    "                })\n",
    "\n",
    "                pd.DataFrame(results).to_csv(results_path, index=False)\n",
    "                results_df = pd.read_csv(results_path)  # reload after saving\n",
    "\n",
    "print(f\"\\nAll results saved to: {results_path}\")\n",
    "display(pd.DataFrame(results))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
