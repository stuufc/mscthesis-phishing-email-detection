{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3ae51d",
   "metadata": {},
   "source": [
    "# DistilBERT 5seed pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff750750",
   "metadata": {},
   "source": [
    "*This code was used in Google Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e525048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_original_array = np.array\n",
    "def safe_array(obj, *args, **kwargs):\n",
    "    if \"copy\" in kwargs and kwargs[\"copy\"] is False:\n",
    "        return np.asarray(obj)\n",
    "    return _original_array(obj, *args, **kwargs)\n",
    "np.array = safe_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc0dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets, load_dataset, Dataset, concatenate_datasets, ClassLabel, DatasetDict\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments, DistilBertTokenizerFast, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada5532",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\n",
    "    \"Al-Subaiey_CEAS\",\n",
    "    \"Al-Subaiey_Enron\",\n",
    "    \"Al-Subaiey_Ling\",\n",
    "    \"Al-Subaiey_SpamAssassin\",\n",
    "    \"Champa_Trec\",\n",
    "    \"Chatuat_Enhancing_Phishing_Detection\",\n",
    "    \"Giri_EnronSpamSubset\",\n",
    "    \"Giri_LingSpam\"]\n",
    "\n",
    "base_path = \"/content/drive/MyDrive/MASTER EXPERIMENTS/original_datasets\"\n",
    "\n",
    "original_datasets = {\n",
    "    name: pd.read_csv(os.path.join(base_path, f\"{name}.csv\"))\n",
    "    for name in dataset_names\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd31c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create combined datasets\n",
    "combined_sets = []\n",
    "\n",
    "for excluded_name, excluded_df in original_datasets.items():\n",
    "    included_dfs = [df for name, df in original_datasets.items() if name != excluded_name]\n",
    "    combined_df = pd.concat(included_dfs, ignore_index=True)\n",
    "\n",
    "    combined_sets.append({\n",
    "        \"name\": f\"combined_without_{excluded_name}\",\n",
    "        \"excluded_name\": excluded_name,\n",
    "        \"train\": combined_df,\n",
    "        \"excluded_test\": excluded_df\n",
    "    })\n",
    "\n",
    "\n",
    "for entry in combined_sets:\n",
    "    train_size = len(entry[\"train\"])\n",
    "    test_size = len(entry[\"excluded_test\"])\n",
    "    print(f\"{entry['name']}: Train size = {train_size}, Test size (excluded) = {test_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b2c43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    texts = [text if isinstance(text, str) else \"\" for text in batch[\"text\"]]\n",
    "    return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "def prepare_dataset(ds):\n",
    "    columns_to_remove = [col for col in ds.column_names if col not in [\"input_ids\", \"attention_mask\", \"label\"]]\n",
    "    if columns_to_remove:\n",
    "        ds = ds.remove_columns(columns_to_remove)\n",
    "    return ds.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93006f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sets = []\n",
    "seed_list = [7, 28, 42, 95, 450]\n",
    "\n",
    "for entry in combined_sets:\n",
    "    name = entry[\"name\"]\n",
    "    excluded_name = entry[\"excluded_name\"]\n",
    "\n",
    "    #tokenize full combined train set\n",
    "    full_train_df = entry[\"train\"][[\"text\", \"label\"]].reset_index(drop=True)\n",
    "    full_ds = Dataset.from_pandas(full_train_df)\n",
    "    tokenized_full = prepare_dataset(full_ds.map(tokenize_function, batched=True))\n",
    "\n",
    "    #tokenize excluded test sets\n",
    "    test_ds = Dataset.from_pandas(entry[\"excluded_test\"][[\"text\", \"label\"]].reset_index(drop=True))\n",
    "    tokenized_test = prepare_dataset(test_ds.map(tokenize_function, batched=True))\n",
    "\n",
    "    #create seed based splits\n",
    "    labels = full_train_df[\"label\"].to_numpy()\n",
    "    indices = np.arange(len(full_train_df))\n",
    "\n",
    "    for seed in seed_list:\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            indices,\n",
    "            test_size=0.3,\n",
    "            stratify=labels,\n",
    "            random_state=seed\n",
    "        )\n",
    "\n",
    "        tokenized_train = tokenized_full.select(train_idx.tolist())\n",
    "        tokenized_val   = tokenized_full.select(val_idx.tolist())\n",
    "\n",
    "        tokenized_sets.append({\n",
    "            \"name\": name,\n",
    "            \"excluded_name\": excluded_name,\n",
    "            \"seed\": seed,\n",
    "            \"train\": tokenized_train,\n",
    "            \"val\": tokenized_val,\n",
    "            \"excluded_test\": tokenized_test\n",
    "        })\n",
    "\n",
    "    print(f\"Prepared dataset '{name}' with {len(seed_list)} seeds.\")\n",
    "\n",
    "print(f\"\\nLoaded and tokenized {len(tokenized_sets)} dataset-seed combinations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea481233",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device.upper()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e18278",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_template = {\n",
    "    \"eval_strategy\": \"epoch\",\n",
    "    \"num_train_epochs\": 6,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"per_device_train_batch_size\": 32,\n",
    "    \"per_device_eval_batch_size\": 64,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"logging_strategy\": \"epoch\",\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"eval_f1\",\n",
    "    \"save_total_limit\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e72132",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_dir = \"/content/drive/MyDrive/MASTER EXPERIMENTS/bert_allvsone_5seed_models\"\n",
    "os.makedirs(bert_model_dir, exist_ok=True)\n",
    "\n",
    "results_dir = \"/content/drive/MyDrive/MASTER EXPERIMENTS/bert_allvsone_5seed_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be84fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to compute metrics for trainer\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds),\n",
    "        \"recall\": recall_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c0245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training function in the all vs one setup\n",
    "def train_model_on_dataset(name, seed, train_ds, val_ds, output_dir=bert_model_dir):\n",
    "    model_output_dir = os.path.join(output_dir, f\"bert_{name}_seed{seed}\")\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(model_output_dir) and os.path.isfile(os.path.join(model_output_dir, \"model.safetensors\")): #check if model already exists but still use it for evaluation (implemented because of session timeouts)\n",
    "        print(f\"Model already exists at: {model_output_dir}.\")\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(model_output_dir) #load the model again\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "        output_dir=model_output_dir,\n",
    "        report_to=\"none\",\n",
    "        seed=seed,\n",
    "        **training_args_template\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        eval_dataset=val_ds,\n",
    "        compute_metrics=compute_metrics\n",
    "        )\n",
    "        return trainer #return the trainer for the evaluation loop\n",
    "\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"[Seed {seed}] Training on: {name}\")\n",
    "\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_dir,\n",
    "        logging_dir=os.path.join(model_output_dir, \"logs\"),\n",
    "        report_to=\"none\",\n",
    "        **training_args_template\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] #stops if no improvements 3x in a row\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    #save model and tokenizer\n",
    "    model.save_pretrained(model_output_dir)\n",
    "    tokenizer.save_pretrained(model_output_dir)\n",
    "\n",
    "    print(f\"[Seed {seed}] Finished training on {name}. Model saved to: {model_output_dir}\")\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd99aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = os.path.join(results_dir, \"all_vs_one_bert_5seed_results.csv\")\n",
    "\n",
    "#load existing results if available\n",
    "if os.path.exists(results_path):\n",
    "    results_df = pd.read_csv(results_path)\n",
    "    print(f\"Loaded existing results with {len(results_df)} entries from {results_path}\")\n",
    "else:\n",
    "    results_df = pd.DataFrame(columns=[\n",
    "        \"Train Dataset\", \"Train Seed\",\n",
    "        \"Test Dataset\",  \"Test Seed\",\n",
    "        \"Evaluation Type\",\n",
    "        \"Train Size\", \"Test Size\",\n",
    "        \"Accuracy\", \"Precision\", \"Recall\", \"F1\"\n",
    "    ])\n",
    "\n",
    "for entry in tokenized_sets:\n",
    "    name = entry[\"name\"]\n",
    "    excluded_name = entry[\"excluded_name\"]\n",
    "    seed = entry[\"seed\"]\n",
    "\n",
    "    print(f\"\\n[Seed {seed}] Preparing dataset: {name}\")\n",
    "\n",
    "    train_ds = entry[\"train\"]\n",
    "    val_ds   = entry[\"val\"]\n",
    "    test_ds  = entry[\"excluded_test\"]\n",
    "\n",
    "    #train or resume model\n",
    "    trainer = train_model_on_dataset(name, seed, train_ds, val_ds)\n",
    "\n",
    "    #internal evaluation\n",
    "    result_exists = (\n",
    "        (results_df[\"Train Dataset\"]   == name) &\n",
    "        (results_df[\"Train Seed\"]      == seed) &\n",
    "        (results_df[\"Test Dataset\"]    == name) &\n",
    "        (results_df[\"Test Seed\"]       == seed) &\n",
    "        (results_df[\"Evaluation Type\"] == \"Internal\")\n",
    "    ).any()\n",
    "\n",
    "    if result_exists:\n",
    "        print(f\"[Seed {seed}] skipping internal evaluation for '{name}' (already in results CSV)\")\n",
    "    else:\n",
    "        print(f\"[Seed {seed}] start internal evaluation: dataset='{name}'\")\n",
    "        val_pred   = trainer.predict(val_ds)\n",
    "        val_preds  = np.argmax(val_pred.predictions, axis=1)\n",
    "        val_labels = val_pred.label_ids\n",
    "\n",
    "        new_row = {\n",
    "            \"Train Dataset\": name,\n",
    "            \"Train Seed\":    seed,\n",
    "            \"Test Dataset\":  name,\n",
    "            \"Test Seed\":     seed,\n",
    "            \"Evaluation Type\": \"Internal\",\n",
    "            \"Train Size\": len(train_ds),\n",
    "            \"Test Size\":  len(val_ds),\n",
    "            \"Accuracy\":   accuracy_score(val_labels, val_preds),\n",
    "            \"Precision\":  precision_score(val_labels, val_preds, zero_division=0),\n",
    "            \"Recall\":     recall_score(val_labels, val_preds,  zero_division=0),\n",
    "            \"F1\":         f1_score(val_labels, val_preds,      zero_division=0),\n",
    "        }\n",
    "\n",
    "        tmp_path = results_path + \".tmp\"\n",
    "        updated_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        updated_df.to_csv(tmp_path, index=False)\n",
    "        os.replace(tmp_path, results_path)\n",
    "        results_df = updated_df\n",
    "        print(f\"[Seed {seed}] Appended INTERNAL result for '{name}'\")\n",
    "\n",
    "    #external evaluation on full excluded dataset\n",
    "    result_exists = (\n",
    "        (results_df[\"Train Dataset\"]   == name) &\n",
    "        (results_df[\"Train Seed\"]      == seed) &\n",
    "        (results_df[\"Test Dataset\"]    == excluded_name) &\n",
    "        (results_df[\"Evaluation Type\"] == \"External\")\n",
    "    ).any()\n",
    "\n",
    "    if result_exists:\n",
    "        print(f\"[Seed {seed}] skipping external evaluation: train='{name}', test='{excluded_name}' (already in results csv).\")\n",
    "    else:\n",
    "        print(f\"[Seed {seed}] start external evaluation: train='{name}', test='{excluded_name}'.\")\n",
    "        test_pred   = trainer.predict(test_ds)\n",
    "        test_preds  = np.argmax(test_pred.predictions, axis=1)\n",
    "        test_labels = test_pred.label_ids\n",
    "\n",
    "        new_row = {\n",
    "            \"Train Dataset\": name,\n",
    "            \"Train Seed\":    seed,\n",
    "            \"Test Dataset\":  excluded_name,\n",
    "            \"Test Seed\":     \"FULL\",\n",
    "            \"Evaluation Type\": \"External\",\n",
    "            \"Train Size\": len(train_ds),\n",
    "            \"Test Size\":  len(test_ds),\n",
    "            \"Accuracy\":   accuracy_score(test_labels, test_preds),\n",
    "            \"Precision\":  precision_score(test_labels, test_preds, zero_division=0),\n",
    "            \"Recall\":     recall_score(test_labels, test_preds,  zero_division=0),\n",
    "            \"F1\":         f1_score(test_labels, test_preds,      zero_division=0),\n",
    "        }\n",
    "\n",
    "        tmp_path = results_path + \".tmp\"\n",
    "        updated_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        updated_df.to_csv(tmp_path, index=False)\n",
    "        os.replace(tmp_path, results_path)\n",
    "        results_df = updated_df\n",
    "        print(f\"[Seed {seed}] Appended external result train='{name}', test='{excluded_name}'\")\n",
    "\n",
    "print(f\"\\nAll seeds completed. Final results saved to: {results_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
