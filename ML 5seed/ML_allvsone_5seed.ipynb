{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b68addd",
   "metadata": {},
   "source": [
    "# ML all vs. one pipeline 5 seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40034985",
   "metadata": {},
   "source": [
    "*Code used in Google Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9f92f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764f798",
   "metadata": {},
   "source": [
    "## Load prepared datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516fd937",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/content/drive/MyDrive/MASTER EXPERIMENTS/ML allvsone 5seed/5seed_cleaned_datasets\"\n",
    "all_datasets = {}\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        name = file.replace(\".csv\", \"\")\n",
    "        df = pd.read_csv(os.path.join(folder_path, file))\n",
    "        all_datasets[name] = df\n",
    "print(f\"Loaded {len(all_datasets)} cleaned datasets\")\n",
    "display(all_datasets.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15badec4",
   "metadata": {},
   "source": [
    "## Load combined (for training) and single datasets (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all combined datasets for training\n",
    "combined_folder = \"/content/drive/MyDrive/MASTER EXPERIMENTS/ML allvsone 5seed/all_vs_one_combined_trainsets\"\n",
    "train_sets = {}\n",
    "\n",
    "for file in os.listdir(combined_folder):\n",
    "    if file.endswith(\".csv\") and file.startswith(\"combined_without_\"):\n",
    "        file_path = os.path.join(combined_folder, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        train_set_name = file.replace(\".csv\", \"\")\n",
    "        train_sets[train_set_name] = df\n",
    "\n",
    "        print(f\"Loaded training set '{train_set_name}' ({df.shape[0]} rows)\")\n",
    "\n",
    "test_sets = all_datasets\n",
    "print(f\"Loaded {len(test_sets)} test sets from all_datasets\")\n",
    "display(test_sets.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cfced8",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15e895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(seed):\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(\n",
    "            C=0.05,  #strong regularization according to SpamEval Benchmarks, reduce overfitting on small datasets\n",
    "            penalty=\"l2\", #standard l2 penalty for text classification\n",
    "            class_weight=\"balanced\", #recommended for imbalanced classes\n",
    "            solver=\"liblinear\", #solver can handle large feature space for text data\n",
    "            max_iter=1000,\n",
    "            random_state=seed #for reproducibility\n",
    "        ),\n",
    "\n",
    "        \"Naive Bayes\": MultinomialNB(\n",
    "            alpha=0.1  #slight smoothing to handle unseen words that are common in phishing emails\n",
    "        ),\n",
    "\n",
    "        \"Random Forest\": RandomForestClassifier(\n",
    "            n_estimators=300,            #300 trees for stable results, recommended on text data\n",
    "            max_depth=None,              #no limit on depth, ensemble reduces overfitting\n",
    "            min_samples_leaf=2,          #avoids very specific leaf nodes (regularization)\n",
    "            max_features=\"sqrt\",         #recommended for high dimensional text-features\n",
    "            class_weight=\"balanced\",     #recommended for imbalanced classes to reduce bias\n",
    "            n_jobs=-2,\n",
    "            random_state=seed\n",
    "        ),\n",
    "\n",
    "        \"Support Vector Machine\": SVC(\n",
    "            kernel=\"linear\",             #recommended for high dimensional TF-IDF features\n",
    "            C=0.05,                      #strong regularization to prevent overfitting on limited data\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=seed\n",
    "        ),\n",
    "\n",
    "        \"Neural Network\": MLPClassifier(\n",
    "            hidden_layer_sizes=(80,),   #according to literature, 80 neurons are enough for text data\n",
    "            activation=\"relu\",          #standard activation function for text data\n",
    "            alpha=0.0005,               #L2 regularization (weights decay) to prevent overfitting\n",
    "            early_stopping=True,        #stops training if validation score does not improve\n",
    "            learning_rate_init=0.001,   #initial learning rate for small neural networks\n",
    "            learning_rate=\"adaptive\",   #learning rate decreases when validation stops improving\n",
    "            max_iter=1000,              #allows convergence for small datasets\n",
    "            random_state=seed\n",
    "        )\n",
    "    }\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5280dc9b",
   "metadata": {},
   "source": [
    "# All vs. one 5seed Pipeline ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eed069",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_list = [7, 28, 42, 95, 450]\n",
    "\n",
    "#output folder for models & splits\n",
    "base_output_dir = \"/content/drive/MyDrive/MASTER EXPERIMENTS/ML allvsone 5seed/ML_allvsone_5seed_output\"\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "#folder for results\n",
    "results_dir = \"/content/drive/MyDrive/MASTER EXPERIMENTS/ML allvsone 5seed/ML_allvsone_5seed_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "#combined results csv\n",
    "all_results_csv_path = os.path.join(results_dir, \"all_vs_one_results_seeds_28_450.csv\")\n",
    "\n",
    "#load existing results if available\n",
    "if os.path.exists(all_results_csv_path):\n",
    "    all_results_df = pd.read_csv(all_results_csv_path)\n",
    "else:\n",
    "    all_results_df = pd.DataFrame(columns=[\"Seed\", \"Trained_On\", \"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"])\n",
    "\n",
    "for seed in seed_list:\n",
    "    print(f\"\\n=== Starting experiments for seed {seed} ===\")\n",
    "\n",
    "    #create output folders per seed\n",
    "    output_dir = os.path.join(base_output_dir, f\"seed_{seed}\")\n",
    "    split_dir = os.path.join(output_dir, \"splits\")\n",
    "    model_dir = os.path.join(output_dir, \"models\")\n",
    "\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    #load models with current seed\n",
    "    models = load_models(seed)\n",
    "\n",
    "    for train_set_name, df_combined in train_sets.items():\n",
    "        print(f\"\\n[Seed {seed}] Preparing dataset: {train_set_name}\")\n",
    "\n",
    "        features = [\n",
    "            \"subject_clean\", \"text_clean\",\n",
    "            \"subject_length\", \"text_length\",\n",
    "            \"num_exclamations\", \"num_uppercase_words\",\n",
    "            \"num_urls\", \"num_special_chars\"\n",
    "        ]\n",
    "        X = df_combined[features].copy()\n",
    "        y = df_combined[\"label\"]\n",
    "\n",
    "        X[\"subject_clean\"] = X[\"subject_clean\"].fillna(\"\").astype(str)\n",
    "        X[\"text_clean\"] = X[\"text_clean\"].fillna(\"\").astype(str)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=seed, stratify=y\n",
    "        )\n",
    "\n",
    "        # save dataset split once per dataset (robust write)\n",
    "        split_path = os.path.join(split_dir, f\"{train_set_name}_split.pkl\")\n",
    "        if not os.path.exists(split_path):\n",
    "            try:\n",
    "                joblib.dump({\n",
    "                    \"X_train\": X_train, \"X_test\": X_test,\n",
    "                    \"y_train\": y_train, \"y_test\": y_test\n",
    "                }, split_path)\n",
    "                print(f\"[Seed {seed}] Successfully saved split for '{train_set_name}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"[Seed {seed}] Error saving split for '{train_set_name}': {e}\")\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            print(f\"[Seed {seed}] Dataset '{train_set_name}': checking status for model '{model_name}' ...\")\n",
    "\n",
    "            #check if result already exists in CSV\n",
    "            result_exists = (\n",
    "                (all_results_df[\"Seed\"] == seed) &\n",
    "                (all_results_df[\"Trained_On\"] == train_set_name) &\n",
    "                (all_results_df[\"Model\"] == model_name)\n",
    "            ).any()\n",
    "            if result_exists:\n",
    "                print(f\"[Seed {seed}] Skipping {model_name} on '{train_set_name}' (already in results CSV)\")\n",
    "                continue\n",
    "\n",
    "            #build model path and check for existing model file\n",
    "            safe_model_name = model_name.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "            model_path = os.path.join(\n",
    "                model_dir, f\"{safe_model_name}__{train_set_name.replace('.csv', '')}.pkl\"\n",
    "            )\n",
    "\n",
    "            if os.path.exists(model_path):\n",
    "                #resume model exists but CSV entry is missing\n",
    "                print(f\"[Seed {seed}] Found existing model file for {model_name} on '{train_set_name}'. Loading and evaluating ...\")\n",
    "                try:\n",
    "                    loaded_pipeline = joblib.load(model_path)\n",
    "                    y_pred = loaded_pipeline.predict(X_test)\n",
    "\n",
    "                    new_result = {\n",
    "                        \"Seed\": seed,\n",
    "                        \"Trained_On\": train_set_name,\n",
    "                        \"Model\": model_name,\n",
    "                        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "                        \"Precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "                        \"Recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "                        \"F1 Score\": f1_score(y_test, y_pred, zero_division=0)\n",
    "                    }\n",
    "                    all_results_df = pd.concat([all_results_df, pd.DataFrame([new_result])], ignore_index=True)\n",
    "\n",
    "                    temp_path = all_results_csv_path + \".tmp\"\n",
    "                    all_results_df.to_csv(temp_path, index=False)\n",
    "                    os.replace(temp_path, all_results_csv_path)\n",
    "\n",
    "                    print(f\"[Seed {seed}] Appended result (resume) for {model_name} on '{train_set_name}'\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"[Seed {seed}] Failed to load existing model for {model_name} on '{train_set_name}'. Will retrain. Error: {e}\")\n",
    "\n",
    "            #train from scratch\n",
    "            if model_name == \"Naive Bayes\":\n",
    "                transformer = ColumnTransformer([\n",
    "                    (\"subject_tfidf\", TfidfVectorizer(\n",
    "                        max_features=2000,\n",
    "                        ngram_range=(1, 2),\n",
    "                        min_df=0.01,\n",
    "                        max_df=0.9,\n",
    "                        sublinear_tf=True,\n",
    "                        norm=None,\n",
    "                        stop_words=None,\n",
    "                        lowercase=True,\n",
    "                        strip_accents=\"unicode\"\n",
    "                    ), \"subject_clean\"),\n",
    "                    (\"text_tfidf\", TfidfVectorizer(\n",
    "                        max_features=5000,\n",
    "                        ngram_range=(1, 2),\n",
    "                        min_df=0.01,\n",
    "                        max_df=0.9,\n",
    "                        sublinear_tf=True,\n",
    "                        norm=None,\n",
    "                        stop_words=None,\n",
    "                        lowercase=True,\n",
    "                        strip_accents=\"unicode\"\n",
    "                    ), \"text_clean\")\n",
    "                ])\n",
    "            else:\n",
    "                transformer = ColumnTransformer([\n",
    "                    (\"subject_tfidf\", TfidfVectorizer(\n",
    "                        max_features=2000,\n",
    "                        ngram_range=(1, 2),\n",
    "                        min_df=0.01,\n",
    "                        max_df=0.9,\n",
    "                        sublinear_tf=True,\n",
    "                        norm=\"l2\",\n",
    "                        stop_words=None,\n",
    "                        lowercase=True,\n",
    "                        strip_accents=\"unicode\"\n",
    "                    ), \"subject_clean\"),\n",
    "                    (\"text_tfidf\", TfidfVectorizer(\n",
    "                        max_features=5000,\n",
    "                        ngram_range=(1, 2),\n",
    "                        min_df=0.01,\n",
    "                        max_df=0.9,\n",
    "                        sublinear_tf=True,\n",
    "                        norm=\"l2\",\n",
    "                        stop_words=None,\n",
    "                        lowercase=True,\n",
    "                        strip_accents=\"unicode\"\n",
    "                    ), \"text_clean\"),\n",
    "                    (\"numerical\", StandardScaler(), [\n",
    "                        \"subject_length\", \"text_length\",\n",
    "                        \"num_exclamations\", \"num_uppercase_words\",\n",
    "                        \"num_urls\", \"num_special_chars\"\n",
    "                    ])\n",
    "                ])\n",
    "\n",
    "            pipeline = Pipeline([\n",
    "                (\"features\", transformer),\n",
    "                (\"clf\", model)\n",
    "            ])\n",
    "\n",
    "            print(f\"[Seed {seed}] START training: model='{model_name}' dataset='{train_set_name}'\")\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            print(f\"[Seed {seed}] FINISH training: model='{model_name}' dataset='{train_set_name}'\")\n",
    "\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "\n",
    "            joblib.dump(pipeline, model_path)\n",
    "            print(f\"[Seed {seed}] Saved model to: {model_path}\")\n",
    "\n",
    "            new_result = {\n",
    "                \"Seed\": seed,\n",
    "                \"Trained_On\": train_set_name,\n",
    "                \"Model\": model_name,\n",
    "                \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "                \"Precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "                \"Recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "                \"F1 Score\": f1_score(y_test, y_pred, zero_division=0)\n",
    "            }\n",
    "            all_results_df = pd.concat([all_results_df, pd.DataFrame([new_result])], ignore_index=True)\n",
    "\n",
    "            temp_path = all_results_csv_path + \".tmp\"\n",
    "            all_results_df.to_csv(temp_path, index=False)\n",
    "            os.replace(temp_path, all_results_csv_path)\n",
    "            print(f\"[Seed {seed}] Appended result for {model_name} on '{train_set_name}'\")\n",
    "\n",
    "print(f\"\\nAll seeds completed. Final results saved to: {all_results_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
