{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9d4042",
   "metadata": {},
   "source": [
    "# ML pipeline 5 seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c9082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40dc282",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55649576",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../Datasets/processed_datasets\"\n",
    "datasets = {}\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        name = file.removesuffix(\".csv\")\n",
    "        df = pd.read_csv(os.path.join(folder_path, file))\n",
    "        datasets[name] = df\n",
    "\n",
    "# correct the label column data types to int\n",
    "for name, df in datasets.items():\n",
    "    df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"label\"]).copy()\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "    datasets[name] = df  #update dictionary\n",
    "    print(f\"{name}: {df['label'].dtype}, unique values: {df['label'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e5489",
   "metadata": {},
   "source": [
    "### Clean datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(text, use_stemming=True):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text) #remove URLs that can confuse TF-IDF vectorizer\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text) #remove special characters\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in ENGLISH_STOP_WORDS] #remove stop words\n",
    "    if use_stemming: #apply stemming\n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "filtered_datasets = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    original_len = len(df)\n",
    "\n",
    "    #remove rows with NaN values in subject or text\n",
    "    df = df[\n",
    "        df[\"subject\"].apply(lambda x: isinstance(x, str) and x.strip() != \"\") &\n",
    "        df[\"text\"].apply(lambda x: isinstance(x, str) and x.strip() != \"\")\n",
    "    ]\n",
    "\n",
    "    #remove duplicates\n",
    "    df = df.dropna(subset=[\"subject\", \"text\"])\n",
    "    before_dupes = len(df)\n",
    "    df = df.drop_duplicates(subset=[\"subject\", \"text\"])\n",
    "    removed_dupes = before_dupes - len(df) #check how many duplicates were removed\n",
    "    if removed_dupes > 0:\n",
    "        print(f\"{name}: {removed_dupes} removed duplicates\")\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    #extract features from subject and text\n",
    "    df[\"subject_length\"] = df[\"subject\"].fillna(\"\").apply(len)\n",
    "    df[\"text_length\"] = df[\"text\"].fillna(\"\").apply(len)\n",
    "    df[\"num_exclamations\"] = df[\"text\"].fillna(\"\").apply(lambda x: x.count(\"!\"))\n",
    "    df[\"num_uppercase_words\"] = df[\"text\"].fillna(\"\").apply(lambda x: sum(1 for w in x.split() if w.isupper() and len(w) > 1))\n",
    "    df[\"num_urls\"] = df[\"text\"].fillna(\"\").apply(lambda x: len(re.findall(r\"http[s]?://\", x)))\n",
    "    df[\"num_special_chars\"] = df[\"text\"].fillna(\"\").apply(lambda x: len(re.findall(r\"[#$%^&*]\", x)))\n",
    "\n",
    "    #cleaning subject and text with clean text function \n",
    "    df[\"subject_clean\"] = df[\"subject\"].fillna(\"\").apply(lambda x: clean_text(x, use_stemming=True))\n",
    "    df[\"text_clean\"] = df[\"text\"].fillna(\"\").apply(lambda x: clean_text(x, use_stemming=True))\n",
    "\n",
    "    #check if the dataset has both labels 0 and 1, at least 100 rows, and at least 20 samples for each class\n",
    "    label_counts = df[\"label\"].value_counts()\n",
    "    has_both_labels = 0 in label_counts and 1 in label_counts\n",
    "    sufficient_rows = len(df) >= 100\n",
    "    class_balance_ok = label_counts.get(0, 0) >= 20 and label_counts.get(1, 0) >= 20\n",
    "\n",
    "    if has_both_labels and sufficient_rows and class_balance_ok:\n",
    "        filtered_datasets[name] = df\n",
    "        print(f\"{name} accepted: {len(df)} rows\")\n",
    "    else:\n",
    "        print(f\"{name} skipped: \"\n",
    "              f\"{'missing class 0 or 1' if not has_both_labels else ''} \"\n",
    "              f\"{'not enough rows' if not sufficient_rows else ''} \"\n",
    "              f\"{'imbalanced classes' if not class_balance_ok else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc344f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show examples from the cleaned datasets\n",
    "for name, df in filtered_datasets.items():\n",
    "    print(f\"\\nExample rows from: {name}:\")\n",
    "    print(df.sample(5, random_state=42)[[\"subject\", \"text\", \"label\", \"subject_clean\", \"text_clean\", \"subject_length\", \"num_urls\", \"num_exclamations\", \"label\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72849a3",
   "metadata": {},
   "source": [
    "### Save cleaned datasets as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f836059",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"5seed_cleaned_datasets\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for name, df in filtered_datasets.items():\n",
    "    filename = os.path.join(output_folder, f\"{name}.csv\")\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ab9ffa",
   "metadata": {},
   "source": [
    "### Show Dataset characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a1c257",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_summary = []\n",
    "\n",
    "for name, df in filtered_datasets.items():\n",
    "    num_rows = len(df)\n",
    "    label_counts = df[\"label\"].value_counts().to_dict()\n",
    "    num_1 = label_counts.get(1, 0)\n",
    "    num_0 = label_counts.get(0, 0)\n",
    "    has_subjects = df[\"subject\"].dropna().apply(lambda x: isinstance(x, str) and x.strip() != \"\").any()\n",
    "\n",
    "    dataset_summary.append({\n",
    "        \"Dataset\": name,\n",
    "        \"Rows\": num_rows,\n",
    "        \"Label 1 (Phishing)\": num_1,\n",
    "        \"Label 0 (Benign)\": num_0,\n",
    "        \"Has Subjects\": \"Yes\" if has_subjects else \"No\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(dataset_summary)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3fb34b",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977439e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(seed): \n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(\n",
    "            C=0.05,  #strong regularization to reduce overfitting on small datasets\n",
    "            penalty=\"l2\", #standard l2 penalty for text classification\n",
    "            class_weight=\"balanced\", #recommended for imbalanced classes\n",
    "            solver=\"liblinear\", #solver can handle large feature space for text data\n",
    "            max_iter=1000,\n",
    "            random_state=seed #for reproducibility\n",
    "        ),\n",
    "\n",
    "        \"Naive Bayes\": MultinomialNB(\n",
    "            alpha=0.1  #slight smoothing to handle unseen words that are common in phishing emails\n",
    "        ),\n",
    "\n",
    "        \"Random Forest\": RandomForestClassifier(\n",
    "            n_estimators=300,            #increased number of trees with the goal of generalization\n",
    "            max_depth=None,              \n",
    "            min_samples_leaf=2,          #avoids very specific leaf nodes to allow better generalization\n",
    "            max_features=\"sqrt\",         #standard\n",
    "            class_weight=\"balanced\",     #recommended for imbalanced classes to reduce bias\n",
    "            n_jobs=-2,\n",
    "            random_state=seed\n",
    "        ),\n",
    "\n",
    "        \"Support Vector Machine\": SVC(\n",
    "            kernel=\"linear\",             #good for TF-IDF features\n",
    "            C=0.05,                      #strong regularization to prevent overfitting on limited data\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=seed\n",
    "        ),\n",
    "\n",
    "        \"Neural Network\": MLPClassifier(\n",
    "            hidden_layer_sizes=(80,),   #according to literature, 80 neurons are enough for text data, ranges vary from 50 to 100\n",
    "            activation=\"relu\",          #standard activation function for text data\n",
    "            alpha=0.0005,               #L2 regularization to prevent overfitting\n",
    "            early_stopping=True,        #stops training if validation score does not improve\n",
    "            learning_rate_init=0.001,   #standard setting, initial learning rate for small neural networks\n",
    "            learning_rate=\"adaptive\",   #learning rate decreases when validation stops improving\n",
    "            max_iter=1000,              #allows convergence for small datasets  \n",
    "            random_state=seed\n",
    "        )\n",
    "    }\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585e054a",
   "metadata": {},
   "source": [
    "# Train / Test models on 5 different seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74afed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [7, 28, 42, 95, 450]\n",
    "all_results = []\n",
    "stored_models = {}\n",
    "splits = {}\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\nSplit with random_state={seed}\")\n",
    "    \n",
    "    for dataset_name, df in filtered_datasets.items():\n",
    "        # select relevant features for input\n",
    "        X = df[[\n",
    "            \"subject_clean\", \"text_clean\",\n",
    "            \"subject_length\", \"text_length\",\n",
    "            \"num_exclamations\", \"num_uppercase_words\",\n",
    "            \"num_urls\", \"num_special_chars\"\n",
    "        ]]\n",
    "        y = df[\"label\"]\n",
    "\n",
    "        #stratified train/test split to preserve label distribution\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=seed, stratify=y\n",
    "        )\n",
    "\n",
    "        #save dataset splits for later use\n",
    "        splits[(dataset_name, seed)] = {\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_train\": y_train,\n",
    "            \"y_test\": y_test\n",
    "        }\n",
    "\n",
    "        #load models with seed-specific random_state\n",
    "        models = load_models(seed)\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            #use TF-IDF for text features only for NB\n",
    "            if isinstance(model, MultinomialNB): #NB requires different configuration (no normalization, no negative values)\n",
    "                transformer = ColumnTransformer([\n",
    "                    (\"subject_tfidf\", TfidfVectorizer(\n",
    "                        max_features=2000, #reduces token amount to 2000 for faster training and less overfitting\n",
    "                        ngram_range=(1, 2), #includes word pairs for better context, good for phishing phrases\n",
    "                        min_df=0.01, #removes words that appear in less than 1% of the emails\n",
    "                        max_df=0.9, #removes words that appear in more than 90% of the emails\n",
    "                        sublinear_tf=True, #applies sublinear term frequency scaling, smoothing for high frequency words\n",
    "                        stop_words=None, #stopwords cleaned in pre-processing\n",
    "                        norm=None, #no normalization for Naive Bayes to treat features as counts\n",
    "                        lowercase=True,\n",
    "                        strip_accents=\"unicode\"\n",
    "                    ), \"subject_clean\"),\n",
    "                    (\"text_tfidf\", TfidfVectorizer(\n",
    "                        max_features=5000,\n",
    "                        ngram_range=(1, 2),\n",
    "                        min_df=0.01,\n",
    "                        max_df=0.9,\n",
    "                        sublinear_tf=True,\n",
    "                        norm=None,\n",
    "                        stop_words=None, \n",
    "                        lowercase=True,\n",
    "                        strip_accents=\"unicode\"\n",
    "                    ), \"text_clean\")\n",
    "                ])\n",
    "            else:\n",
    "                #for all other models use TF-IDF + scaled numerical features\n",
    "                transformer = ColumnTransformer([\n",
    "                    (\"subject_tfidf\", TfidfVectorizer(\n",
    "                        max_features=2000,\n",
    "                        ngram_range=(1, 2),\n",
    "                        min_df=0.01,\n",
    "                        max_df=0.9,\n",
    "                        sublinear_tf=True,\n",
    "                        norm=\"l2\", #L2 normalization for all other models (transforms document vectors to unit length)\n",
    "                        stop_words=None,\n",
    "                        lowercase=True,\n",
    "                        strip_accents=\"unicode\"\n",
    "                    ), \"subject_clean\"),\n",
    "                    (\"text_tfidf\", TfidfVectorizer(\n",
    "                        max_features=5000,\n",
    "                        ngram_range=(1, 2),\n",
    "                        min_df=0.01,\n",
    "                        max_df=0.9,\n",
    "                        sublinear_tf=True,\n",
    "                        norm=\"l2\",\n",
    "                        stop_words=None,\n",
    "                        lowercase=True,\n",
    "                        strip_accents=\"unicode\"\n",
    "                    ), \"text_clean\"),\n",
    "                    (\"numerical\", StandardScaler(), [ #engineered numerical features from raw email data\n",
    "                        \"subject_length\", \"text_length\",\n",
    "                        \"num_exclamations\", \"num_uppercase_words\",\n",
    "                        \"num_urls\", \"num_special_chars\"\n",
    "                    ])\n",
    "                ])\n",
    "\n",
    "            #build pipeline with preprocessing (column transformer) and model, pipeline object ensures no data leakage between train and test set\n",
    "            pipeline = Pipeline([\n",
    "                (\"features\", transformer),\n",
    "                (\"clf\", model)\n",
    "            ])\n",
    "\n",
    "            #train model on training set\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            print(f\"{model_name} trained on {dataset_name} with seed {seed}\")\n",
    "\n",
    "            #evaluate on test set\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "\n",
    "            #store trained model pipeline for later use\n",
    "            stored_models[(dataset_name, model_name, seed)] = pipeline\n",
    "\n",
    "            #save performance metrics\n",
    "            all_results.append({\n",
    "                \"Dataset\": dataset_name,\n",
    "                \"Model\": model_name,\n",
    "                \"Seed\": seed,\n",
    "                \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "                \"Precision\": precision_score(y_test, y_pred),\n",
    "                \"Recall\": recall_score(y_test, y_pred),\n",
    "                \"F1 Score\": f1_score(y_test, y_pred)\n",
    "            })\n",
    "\n",
    "export_dir = \"v2_5seed_model_exports\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "#save all trained models to pkl file for later use\n",
    "joblib.dump(stored_models, os.path.join(export_dir, \"stored_models.pkl\"))\n",
    "print(f\"Stored_models saved to '{export_dir}/stored_models.pkl'\")\n",
    "\n",
    "#save train/test splits\n",
    "joblib.dump(splits, os.path.join(export_dir, \"splits.pkl\"))\n",
    "print(f\"Splits saved to '{export_dir}/splits.pkl'\")\n",
    "\n",
    "#results df with all metrics\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "#export results to csv\n",
    "results_df.to_csv(os.path.join(export_dir, \"v2_5seed_model_results.csv\"), index=False)\n",
    "print(f\"v2_5seed_model_results.csv saved to '{export_dir}/v2_5seed_model_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d6161",
   "metadata": {},
   "source": [
    "### Load results from csv for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13be26cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(os.path.join(model_dir, \"v2_5seed_model_results.csv\"))\n",
    "print(f\"Loaded results_df with shape: {results_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fd14dd",
   "metadata": {},
   "source": [
    "### Re-construct dataset splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055488c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_export_dir = \"v2_5seed_splits\"\n",
    "os.makedirs(split_export_dir, exist_ok=True)\n",
    "\n",
    "seeds = [7, 28, 42, 95, 450]\n",
    "\n",
    "#features used for model training\n",
    "feature_columns = [\n",
    "    \"subject_clean\", \"text_clean\",\n",
    "    \"subject_length\", \"text_length\",\n",
    "    \"num_exclamations\", \"num_uppercase_words\",\n",
    "    \"num_urls\", \"num_special_chars\"\n",
    "]\n",
    "\n",
    "#loop through each seed and dataset to create train/test splits\n",
    "for seed in seeds:\n",
    "    for dataset_name, df in filtered_datasets.items():\n",
    "        #select features and labels\n",
    "        X = df[feature_columns]\n",
    "        y = df[\"label\"]\n",
    "\n",
    "        #perform the same stratified split used in model training\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=seed, stratify=y\n",
    "        )\n",
    "\n",
    "        #store the split in a dictionary\n",
    "        split_dict = {\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_train\": y_train,\n",
    "            \"y_test\": y_test\n",
    "        }\n",
    "\n",
    "        #save to separate file for each dataset and seed combination\n",
    "        split_filename = f\"{dataset_name}_seed{seed}_split.pkl\"\n",
    "        joblib.dump(split_dict, os.path.join(split_export_dir, split_filename))\n",
    "        print(f\"Split saved: {split_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778287b4",
   "metadata": {},
   "source": [
    "### Load trained models and dataset splits for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ccdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"v2_5seed_model_exports\"\n",
    "split_dir = \"v2_5seed_splits\"\n",
    "\n",
    "#load trained models\n",
    "stored_models = joblib.load(os.path.join(model_dir, \"stored_models.pkl\"))\n",
    "print(f\"Loaded {len(stored_models)} trained models from '{model_dir}'.\")\n",
    "\n",
    "#load train/test splits\n",
    "splits = {}\n",
    "\n",
    "for file in os.listdir(split_dir):\n",
    "    if file.endswith(\"_split.pkl\"):\n",
    "        parts = file.replace(\"_split.pkl\", \"\").split(\"_seed\")\n",
    "        dataset_name = parts[0]\n",
    "        seed = int(parts[1])\n",
    "        path = os.path.join(split_dir, file)\n",
    "        splits[(dataset_name, seed)] = joblib.load(path)\n",
    "\n",
    "print(f\"Loaded {len(splits)} dataset splits from '{split_dir}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
